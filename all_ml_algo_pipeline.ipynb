{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbc1176-2214-4334-bc27-e73eff0895f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error , classification_report\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier , DecisionTreeRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee16cde6-d532-4d2a-9873-fa59f1d6602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e5858e9-7a5f-45d0-ae18-4a89d4c80c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X_classification = data.data\n",
    "y_classification = data.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_classification, y_classification, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24d854b6-0423-49c3-9e66-67f548de7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESSION DATA\n",
    "# Generate synthetic regression dataset\n",
    "X_regression, y_regression = make_regression(n_samples=150, n_features=2, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_regression, y_regression, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a5d74f0-e2c0-4a3c-9a50-0243a84fce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Best Parameters: {'model__C': 1, 'model__max_iter': 100}\n",
      "Logistic Regression Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        19\n",
      "  versicolor       1.00      1.00      1.00        13\n",
      "   virginica       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Decision Tree Best Parameters: {'model__max_depth': None, 'model__min_samples_split': 10}\n",
      "Decision Tree Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        19\n",
      "  versicolor       1.00      1.00      1.00        13\n",
      "   virginica       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Random Forest Best Parameters: {'model__max_depth': None, 'model__n_estimators': 50}\n",
      "Random Forest Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        19\n",
      "  versicolor       1.00      1.00      1.00        13\n",
      "   virginica       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "SVM Best Parameters: {'model__C': 10, 'model__kernel': 'linear'}\n",
      "SVM Accuracy: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        19\n",
      "  versicolor       1.00      0.92      0.96        13\n",
      "   virginica       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "K-Nearest Neighbors Best Parameters: {'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
      "K-Nearest Neighbors Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        19\n",
      "  versicolor       1.00      1.00      1.00        13\n",
      "   virginica       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Gradient Boosting Best Parameters: {'model__learning_rate': 0.01, 'model__n_estimators': 50}\n",
      "Gradient Boosting Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        19\n",
      "  versicolor       1.00      1.00      1.00        13\n",
      "   virginica       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Define parameter grids for classification models\n",
    "classification_param_grid = {\n",
    "    'Logistic Regression': {\n",
    "        'model__C': [0.1, 1, 10],\n",
    "        'model__max_iter': [100, 200]\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model__max_depth': [None, 10, 20, 30],\n",
    "        'model__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model__C': [0.1, 1, 10],\n",
    "        'model__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'model__n_neighbors': [3, 5, 7],\n",
    "        'model__weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define models for classification\n",
    "classification_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Run GridSearchCV for each classification model\n",
    "for name, model in classification_models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Add scaler to pipeline\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    grid = GridSearchCV(pipeline, classification_param_grid[name], cv=5, scoring='accuracy')\n",
    "    grid.fit(X_train_clf, y_train_clf)\n",
    "    \n",
    "    print(f\"{name} Best Parameters: {grid.best_params_}\")\n",
    "    predictions = grid.predict(X_test_clf)\n",
    "    accuracy = accuracy_score(y_test_clf, predictions)\n",
    "    print(f\"{name} Accuracy: {accuracy:.2f}\")\n",
    "    print(classification_report(y_test_clf, predictions, target_names=data.target_names))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0c7fd6-bbc2-446f-86f8-9c97ec2cdb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Best Parameters: {}\n",
      "Linear Regression MSE: 0.01\n",
      "Decision Tree Regressor Best Parameters: {'model__max_depth': None, 'model__min_samples_split': 5}\n",
      "Decision Tree Regressor MSE: 184.01\n",
      "Random Forest Regressor Best Parameters: {'model__max_depth': None, 'model__n_estimators': 200}\n",
      "Random Forest Regressor MSE: 123.45\n",
      "Gradient Boosting Regressor Best Parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 200}\n",
      "Gradient Boosting Regressor MSE: 110.96\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grids for regression models\n",
    "regression_param_grid = {\n",
    "    'Linear Regression': {},\n",
    "    'Decision Tree Regressor': {\n",
    "        'model__max_depth': [None, 10, 20, 30],\n",
    "        'model__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'Random Forest Regressor': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'Gradient Boosting Regressor': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define models for regression\n",
    "regression_models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting Regressor\": GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Run GridSearchCV for each regression model\n",
    "for name, model in regression_models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Add scaler to pipeline\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    grid = GridSearchCV(pipeline, regression_param_grid[name], cv=5, scoring='neg_mean_squared_error')\n",
    "    grid.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    print(f\"{name} Best Parameters: {grid.best_params_}\")\n",
    "    predictions = grid.predict(X_test_reg)\n",
    "    mse = mean_squared_error(y_test_reg, predictions)\n",
    "    print(f\"{name} MSE: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37504e4a-f300-4fab-a103-a5695575bd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
